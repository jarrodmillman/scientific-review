.. note::

    `Beyond open access: visions for open evaluation of scientific papers by
    post-publication peer review
    <http://www.frontiersin.org/Computational%20Neuroscience/specialtopics/beyond_open_access__visions_fo/137>`_
    
    #. Open evaluation is defined as `an ongoing post-publication process of
       transparent peer review and rating of papers`
    
    #. They are looking for papers that "focus on constructive ideas and
       comprehensive designs for open evaluation systems."
    
    #. They raise a number of interesting questions, which we should keep in
       mind as we work on this article:
    
       - Should the reviews and ratings be entirely transparent, or should some
         aspects be kept secret?
        
       - Should other information, such as paper downloads be included in the
         evaluation?
        
       - How can scientific objectivity be strengthened and political
         motivations weakened in the future system?
        
       - Should the system include signed and authenticated reviews and
         ratings?
        
       - Should the evaluation be an ongoing process, such that promising
         papers are more deeply evaluated?
        
       - How can we bring science and statistics to the evaluation process
         (e.g. should rating averages come with error bars)?
        
       - How should the evaluative information about each paper (e.g. peer
         ratings) be combined to prioritize the literature?
        
       - Should different individuals and organizations be able to define their
         own evaluation formulae (e.g.  weighting ratings according to different
         criteria)?
        
       - How can we efficiently transition toward the future system?

:author: K. Jarrod Millman
:email: millman@berkeley.edu
:institution: University of California Berkeley

:author: Satra Ghosh
:email: 
:institution: MIT

:author: Arno Klein
:email: 
:institution: Columbia University

-------------------------------------------------------------
A continuous, transparent peer review and paper rating system
-------------------------------------------------------------

.. class:: abstract

   A short version of the long version that is way too long to be written as a
   short version anyway.

Introduction
------------

- look at code review process and NumPy documentation editor
- look at Wikipedia's review process
- the idea of a living, growing document, rather than a series of articles each with the same tedious introduction
- scientist typically have a few scientific projects -- more like a software project


Learning from open source software projects to improve scientific review
------------------------------------------------------------------------

Satrajit S. Ghosh (MIT), Arno Klein (Columbia University), Brian Avants (University of Pennsylvania), K. Jarrod Millman (University of California, Berkeley)

Over the last decade, scientists, institutions, publishers and funding
agencies have made tremendous strides in the way scientific research
is disseminated and accessed. With the increase of open-access
journals, availability of articles on PubMed Central, arXiv.org and
elsewhere on the Internet, larger quantities of scientific information
are available than ever before. However, the increasingly
interdisciplinary nature of research and limited availability of
expert reviewers has resulted in high variability in the quality and
speed of the review process. This has led to an even greater
importance being placed on which journal publishes an article and
bibliometrics rather than on the content of the article. The aim of
this paper is to propose an alternate open evaluation framework for
peer review.

In an ideal world, science should be collaborative, open, repeatable
and efficient. The intent of the current “peer review” system is to
enhance the collaborative aspect of science by improving the quality
of submitted manuscripts through critical feedback from one’s
peers. However, given the massive influx of articles and limited time
and number of reviewers, there is a tremendous pressure to retain only
the very “best” for a given journal. We propose that in this
technological age with instant access to information and social
networks, scientific publishing can draw from the ideas, experience
and technology available for code review in open source software
projects.

First, use a distributed review process instead of restricting it to a
few reviewers. In any complex software project, there are specialists
who focus on certain components of the software. However, code review
is not limited to specialists. When multiple eyes look at code, the
code improves, bugs are caught, and all participants are encouraged to
write better code. Opening up scientific reviews to the community will
also ensure that the people most interested and knowledgeable on a
topic review it, thereby speeding up the review process. Furthermore,
the interdisciplinary papers today require far more than two to three
reviewers to adequately spot problems.  Second, ensure an open and
recorded discourse between authors and reviewers. Although certain
journals have an interactive discussion before a paper is accepted,
the discussion is still behind closed doors and limited to a small set
of reviewers. An open and recorded review ensures that there is a
timestamp on the work that has been done, an acknowledgement of who
performed the research and the possibility of rectifying errors early
on in the process. Such discourse can itself be used to quantitatively
assess the importance of a submission. Third, make data and software
used for the research be available as part of the submission
process. This not only ensures transparency and helps reviewers but
will also enhance reproducibility and encourage method reuse. We will
demonstrate how these ideas can improve the quality of published
articles in the highly interdisciplinary field of neuroimaging.

It is in everyone’s scientific interest that every reviewed article is
the best that it can be. An open review process can improve the
quality of articles and research through constructive criticism, and
reduce the time period between initial submission and general
acceptance of an article. In the long run, the review process need not
be limited to publication, but can be engaged throughout the process
of research, from inception through planning, execution, and
documentation. This not only facilitates collaborative research, but
also ensures that the best decisions are taken at every stage in the
evolution of a project.
