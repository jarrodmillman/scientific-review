.. note::

    `Beyond open access: visions for open evaluation of scientific papers by
    post-publication peer review
    <http://www.frontiersin.org/Computational%20Neuroscience/specialtopics/beyond_open_access__visions_fo/137>`_
    
    #. Open evaluation is defined as `an ongoing post-publication process of
       transparent peer review and rating of papers`
    
    #. They are looking for papers that "focus on constructive ideas and
       comprehensive designs for open evaluation systems."
    
    #. They raise a number of interesting questions, which we should keep in
       mind as we work on this article:
    
       - Should the reviews and ratings be entirely transparent, or should some
         aspects be kept secret?
        
       - Should other information, such as paper downloads be included in the
         evaluation?
        
       - How can scientific objectivity be strengthened and political
         motivations weakened in the future system?
        
       - Should the system include signed and authenticated reviews and
         ratings?
        
       - Should the evaluation be an ongoing process, such that promising
         papers are more deeply evaluated?
        
       - How can we bring science and statistics to the evaluation process
         (e.g. should rating averages come with error bars)?
        
       - How should the evaluative information about each paper (e.g. peer
         ratings) be combined to prioritize the literature?
        
       - Should different individuals and organizations be able to define their
         own evaluation formulae (e.g.  weighting ratings according to different
         criteria)?
        
       - How can we efficiently transition toward the future system?

:author: K. Jarrod Millman
:email: millman@berkeley.edu
:institution: University of California Berkeley

:author: Satra Ghosh
:email: 
:institution: MIT

:author: Arno Klein
:email: 
:institution: Columbia University

-------------------------------------------------------------
A continuous, transparent peer review and paper rating system
-------------------------------------------------------------

.. class:: abstract

   A short version of the long version that is way too long to be written as a
   short version anyway.

Introduction
------------

- look at code review process and NumPy documentation editor
- look at Wikipedia's review process
- the idea of a living, growing document, rather than a series of articles each with the same tedious introduction
- scientist typically have a few scientific projects -- more like a software project

Over the last decade, scientists, institutions, publishers and funding agencies
have made tremendous strides in the way scientific research output is
disseminated and accessed. With the increase of open-access journals, the
increasing availability of articles on PubMed Central, arXiv.org and in general
on the Internet, larger quantities of scientific information are available than
ever before. However, the increasing interdisciplinary nature of research, the
limited set of peer-reviewers assigned to an article, and the limited
availability of expert reviewers has resulted in increased variability in the
quality of available information. This has led to an even greater importance
being placed on which journal publishes an article rather than on the content
itself. The aim of this paper is to identify potential problems with the current
review system and to propose an alternate open evaluation framework for peer
review.

In an ideal world, science should be collaborative, open, repeatable and
efficient. The intent of the current “peer review” system is to enhance the
collaborative aspect of science by improving the quality of submitted manuscript
through constructive judgment of one’s peers. However, given the massive influx
of articles and limited time of reviewers, there is a tremendous pressure to
retain only the very “best” for a given journal. We propose that in this
technological age with instant access to information and social networks,
scientific publishing can draw from the ideas, experience and the technology
available for code review in open source projects.

First, reviews need not be restricted to a select few reviewers, but rather
encourage a distributed review process. In any complex software project, there
are specialists who focus on certain components of the software. However, code
review is not limited to the specialists. Having multiple eyes look at the code
improves it and also encourages reviewers themselves to write better
code. Opening up scientific reviews to the community will ensure that the people
most interested and knowledgeable in the topic review it thereby speeding up the
review process.  Second, there is a fair bit of open communication and
discussion over code review before major code changes get accepted. Although
certain journals have an interactive discussion before a paper is accepted, the
discussion is still behind closed doors and limited to a small set of
reviewers. The interdisciplinary papers today require more than two to three
reviewers to adequately spot problems.  Finally, having an open review and
recording it ensures that there is a timestamp on the work that has been done,
an acknowledgement of who performed the research and the possibility of
capturing errors early on in the process. Having an open review process allows
for scientific discourse which appears to be significantly reduced in today’s
publish or perish mentality. Such scientific discourse or contributions could
itself be seen as a quantitative assessment of the importance of the research.

It is in everybody’s scientific interest that every article is the best that it
can be. An open review process can improve the quality of articles submitted,
can improve the research through constructive criticism and can improve the time
period between initial submission and general acceptance of the article.
